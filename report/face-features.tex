% vim:ft=tex
% rubber: module xelatex

\subsection{Face features extraction}
\label{sec:features}

The first scale-invariant feature extraction algorithm was SIFT, described in \cite{SIFT}. SIFT is designed to be able to reliably detect features in altered (e.g. contrast-shifted, re-oriented or scaled) images. SIFT uses a database of training images with identified features to attempt to classify objects in new images based on their feature vectors. Image features are assumed to be the maxima and minima of the difference of Gaussians applied to the images. Outliers are discovered and removed by comparing each image feature with the model, verified by least squares.

The SURF extractor, described in \cite{SURF}, is an attempt to improve upon SIFT by speeding up computation without loss of computation quality or robustness. SURF achieves this improvement by convolving images with integral images, using a Hessian matrix-based detector, using a distribution-based descriptor, and simplifying to only 64 dimensions (see \cite{SURF}). An open-source implementation of this algorithm is OpenSURF, described in \cite{OpenSURF}. OpenCV also includes an implementation of the SURF algorithm. We use both these forms in our program, and add our own implementation of SURF. Our implementation is inspired by (and therefore structured similarly to) the OpenSURF implementation.

\subsubsection{Implementation notes}
Our implementation uses the response layers and filter sizes from \cite{SURF}. This limits the range of possible values for the 'octaves' and 'intervals' parameters to (1-5) and (3-4) respectively. The OpenSURF implementation simply ignores the 'intervals' parameter in its calculations. The process of estimating the determinant of the Hessian matrix from the filter response values requires a weight for the $D_{xy}$ direction. We follow \cite{SURF} in setting this weight to $0.9$.

The three implementations (ours, OpenCV and OpenSURF) use different threshold values for the algorithm. It appears (with the caveat that it is difficult to tell because the code is quite convoluted) that OpenCV does not area-normalise the values. This is likely the reason for the discrepancy between our implementation and that of OpenCV. OpenSURF represents greyscale values as floating point values between 0 and 1, whereas we represent them as int values between 0 and 255, which should account for the second discrepancy between threshold values.

The interpolation between points to get subpixel accuracy for interest points follows the method in \cite{inv-features}; this essentially amounts to solving a linear system using pixel differences as approximations for the derivatives. In \cite{SURF}, it is implied that this process should be used to iteratively interpolate keypoint locations, and discard those that do not converge. However, both OpenCV and OpenSURF appear to simply use the interpolation deltas to discard keypoints that interpolate to a different point, rather than attempting to iterate towards convergence. Our program follows these implementations in abandoning the iterative paradigm. We found, though, that using interpolation for subpixel accuracy \emph{at all} appears to make our implementation 'worse' (in that its results diverge from that of the OpenCV and OpenSURF implementations). Why this should be the case is unclear.

\subsubsection{Experimental process}

\paragraph{Experimental parameters.}
To test the capability of our own algorithm implementation, we ran it on six images of human faces. Credit is given to the Massachusetts Institute of Technology and to the Center for Biological and Computational Learning for providing the database of facial images; see \cite{database}. The forward-facing images subset of the MIT-CBCL database was used. We first manually extracted features from the original images, identifying 10 interest points (eye centres, eye corners, mouth corners, mouth centre, tip of nose) in each case. 
For each face, we tested 40 threshold values (from 1 to 196, differing by 5 in each case). We noted the number of interest points correctly identified as features (counting a hit within 15 pixels of the manually identified feature as a success). As well as these true positives, we also noted the number of false negatives (interest points not identified as features) and false positives (other points incorrectly identified as features).

\paragraph{Experimental results.}
(INSERT CHARTS HERE)


