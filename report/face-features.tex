% vim:ft=tex
% rubber: module xelatex

\subsection{Face features extraction}
\label{sec:features}

The first scale-invariant feature extraction algorithm was SIFT, described in \cite{SIFT}. SIFT is designed to be able to reliably detect features in altered (e.g. contrast-shifted, re-oriented or scaled) images. SIFT uses a database of training images with identified features to attempt to classify objects in new images based on their feature vectors. Image features are assumed to be the maxima and minima of the difference of Gaussians applied to the images. Outliers are discovered and removed by comparing each image feature with the model, verified by least squares.

The SURF extractor, described in \cite{SURF}, is an attempt to improve upon SIFT by speeding up computation without loss of computation quality or robustness. SURF supposedly achieves this improvement by convolving images with integral images, using a Hessian matrix-based detector, using a distribution-based descriptor, and simplifying to only 64 dimensions (see \cite{SURF}). An open-source implementation of this algorithm is OpenSURF, described in \cite{OpenSURF}. OpenCV also includes its own implementation of the SURF algorithm. We integrated both these versions into our application, and have also added our own implementation of SURF. Our implementation is inspired by (and is therefore structured similarly to) the OpenSURF implementation.

\subsubsection{Implementation notes}
Our implementation uses the response layers and filter sizes from \cite{SURF}. This limits the range of possible values for the 'octaves' and 'intervals' parameters to (1-5) and (3-4) respectively. The OpenSURF implementation simply ignores the 'intervals' parameter in its calculations. The process of estimating the determinant of the Hessian matrix from the filter response values requires a weight for the $D_{xy}$ direction. We follow \cite{SURF} in setting this weight to $0.9$.

Note that the three implementations (ours, OpenCV and OpenSURF) use different threshold values for the algorithm. It appears\footnote{with the caveat that it is difficult to tell because the OpenCV code is rather convoluted.} that OpenCV does not area-normalise the threshold values. This is very likely the reason for the discrepancy between our implementation and that of OpenCV. OpenSURF represents greyscale values as floating point values between 0 and 1, whereas we represent them as int values between 0 and 255. This would appear to account for the discrepancy between our threshold values and those of OpenSURF.

The interpolation between points to get subpixel accuracy for interest points follows the method in \cite{inv-features}. This method essentially amounts to solving a linear system using pixel differences as approximations for the derivatives. In \cite{SURF}, it is implied that this process should be used to iteratively interpolate keypoint locations, and discard those that do not converge. However, both OpenCV and OpenSURF appear to simply use the interpolation deltas to discard keypoints that interpolate to a different point, rather than attempting to iterate towards convergence. Our program follows these implementations in abandoning the iterative paradigm. We found, though, that using interpolation for subpixel accuracy \emph{at all} appears to make our implementation 'worse', in as far as it causes the output of our algorithm to diverge from that of the OpenCV and OpenSURF implementations. Why this should be the case is unclear.

\subsubsection{Experimental process}

\paragraph{Experimental parameters.}
To test the capability of our own SURF algorithm implementation, we ran it on six images of human faces. Credit is given to the Massachusetts Institute of Technology and to the Center for Biological and Computational Learning for providing the database of facial images; see \cite{database}. The forward-facing images subset of the MIT-CBCL database was used. We first manually extracted features from the original images, identifying 10 interest points (eye centres, eye corners, mouth corners, mouth centre, tip of nose) in each case. 
For each face, we tested 40 threshold values (from 1 to 196, increasing by a gradation of 5 each time). We noted the number of interest points correctly identified as features; for this purpose we count an interest point that ``hits" within 15 pixels of the manually identified feature as a success). As well as these true positives, we also investigated the number of false negatives (interest points the algorithm fails to identify as features) and false positives (other points which the algorithm incorrectly identifies as features\footnote{i.e. they do not lie within 15 pixels of a manually identified feature point.}).

\begin{figure}
  \centering
  \subfloat[Face image 1]{ \label{fig:face-1} \includegraphics[width=0.4\textwidth]{figures/face1} }
  \subfloat[Face image 2]{ \label{fig:face-2} \includegraphics[width=0.4\textwidth]{figures/face2} }
  \subfloat[Face image 3]{ \label{fig:face-3} \includegraphics[width=0.4\textwidth]{figures/face3} }
  \subfloat[Face image 4]{ \label{fig:face-4} \includegraphics[width=0.4\textwidth]{figures/face4} }
  \subfloat[Face image 5]{ \label{fig:face-5} \includegraphics[width=0.4\textwidth]{figures/face5} }
  \subfloat[Face image 6]{ \label{fig:face-6} \includegraphics[width=0.4\textwidth]{figures/face6} }
  \caption[Bluh]{Face features identified by our implementation of the SURF algorithm. The graph shows ??? for six different test images. How? Bars? etc.}
  \label{fig:face-features-hits}
\end{figure}

\paragraph{Experimental results.}
We present the results of our testing in figure~\ref{fig:face-features-hits}.

