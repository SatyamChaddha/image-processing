% vim:ft=tex
% rubber: module xelatex
\subsection{Camera calibration}
For the camera calibration part of the application, we have
implemented the calibration algorithm from \cite{TSAI}, which is a
classical and often cited calibration method. The article encompasses
several different methods of calibration; we are implementing the
method that the article describes as ``the monoview non-coplanar
case''. That is, we the calibration is performed from a single image
of a calibration object that has calibration points in several (world)
planes.

Our calibration object can be seen in figure~\ref{fig:calib-object}.
It consists of two adjacent faces of a cube, each of which has a
number of circular calibration points. The left face has 35 points,
and the right face 28. The distance between point centres is 1,5
inches. The world coordinate system is chosen to be a right-handed
coordinate system, centred at the bottom corned of the cube, so the
left face corresponds to the YZ plane, and the right face corresponds
to the XZ plane.

\begin{figure}[HBO]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/calibration-object}
  \caption{Calibration object. The left face has 35 calibration points
    (the centres of the circles), and the right face has 28. The
    distance between point centres is 1,5 inches. The world coordinate
    system is chosen to be a right-handed coordinate system, centred
    at the bottom corned of the cube, so the left face corresponds to
    the YZ plane, and the right face corresponds to the XZ plane.}
  \label{fig:calib-object}
\end{figure}

\subsubsection{Implementation notes}
The implementation overall follows the procedure laid out in
\cite{TSAI}. However, a few notes on the implementation are
appropriate:

\paragraph{Implemented parts of the algorithm}
As mentioned, we implement the calibration method that Tsai refers to
as ``the monoview non-coplanar case''. The calibration function derives
the transformation matrix, composed of the rotation matrix, $R$ and
the translation matrix $T$, find the scale factor $s_x$, and the
intrinsic camera values $f$ (focal length) and $\kappa$ (radial
distortion parameter). The radial distortion is calculated as far as
$\kappa_1$. The literature suggests that this is sufficient for
reasonably high accuracy using cameras without significant lens
distortion \cite{algebraic-distortion}.

\paragraph{Mapping of image coordinates to world coordinates}
The world coordinates of the points are measured manually, in units of
inches. These are given as input to the program in the form of a
space-separated text file. The image coordinate points are found by
first running the SURF feature point extraction algorithm described in
section~\ref{sec:features} on the input image, with a relatively high
threshold value (500). The feature points detected using this
algorithm are shown to the user on a binary image (obtained using
adaptive thresholding -- see the description of the segmentation
algorithms in section~\ref{sec:segmentation}), who then has the option
of removing and adding points.

These user-corrected points are fed to the second stage of the
calibration algorithm. This stage assumes that there are exactly 63
points selected on the image, and furthermore takes as input exactly
63 3D points (corresponding to the 63 circles on the calibration
object). The image points are first corrected to be closer to the
centre of the circle. This is done by flood filling the binary image
from each point with a threshold of 0 (i.e. finding all pixels of the
same colour as the point), and taking the average x and y values from
this region. This means that the user does not have to select points
that are exactly at the centre of the calibration region, but can
click anywhere within it.

Following this correction, the image points are mapped to the real
world coordinate points. This is done by a brute-force algorithm
relying on the fact that
\begin{inparaenum}
  \item the right and left faces are entirely disjoint in the
    horizontal direction, i.e. no points on the right face have
    x-coordinates lower than any points on the left face and
  \item the top and bottom outermost points in each face are the
    points closest to the respective image corners on that side of the
    image.
\end{inparaenum}
This algorithm does impose some limitations on the possible viewing
angles of the calibration object. For example, if the image is rotated
by enough to make the faces overlap in the horizontal direction, the
first assumption breaks down and the image point to world point
mapping fails.

\paragraph{Back-projection}

To test the error ratio of the calibration, we reverse the direction
of projection, by projecting the rays from the camera through the
image coordinate points onto the corresponding face planes in world
coordinates (taking into account the (reversed) calibration
parameters). This allows us to measure the distance between these
derived world coordinates and the known ones as an error radius.

\paragraph{Implementation problems}

We have encountered a few problems in our implementation. Primarily,
it was not completely obvious to us that, in going from the coplanar
to the non-coplanar cases, equation (15) in \cite{TSAI} has to be
re-derived from the earlier equation (8b), because the it is no longer
true that $z_w=0$. Also, it proved tricky to get the back-projection
working correctly.

%Calibration experiments:\\
%
%-> On eight images (2 low grade, 2 high grade, 2 low grade distorted,
%2 high grade distorted)...\\
%
%-> Compare backprojection accuracy... mean, variance, worst hit, best
%hit, breakdown into (x,y,z) \\
%
%-> Compare kappa calculated (should be higher with lens distortion...
%but using actual fisheye distortion may be so high that the polynomial
%kappa model is unsuitable for it, as \cite{straightlines} (I believe
%ot was) suggests can happen).\\
%
%-> Subjectively, look at the resulting translation and rotation
%matrices and consider what they mean, as well as the sx and
%focalLength.\\
%
