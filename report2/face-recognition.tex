% vim:ft=tex
% rubber: module xelatex

\subsection{Face recognition}
\label{sec:face-rec}
We have implemented face recognition based on PCA analysis of the image pixel
values (the holistic approach). Face recognition is effectively the culmination of all our work so far.

Our first step is to preprocess the image database. Images must first be rectified (see section~\ref{sec:rectification}). Preprocessing ensures that a fixed triple of face features (the outer corners of the eyes, and the point between the base of the nose and the lip) are in the same positions for all images. For further details, see section~\ref{sec:preprocess} below. PCA is performed on these processed images.

Up to six channels of data from each image can be used: values for red, green and blue, values for hue and saturation, and depth values. Depth values are created by running our stereo matching algorithm (see section~\ref{sec:stereo}) on the preprocessed images to create equivalent-sized depth maps.\footnote{ It is important at this step to use a fixed multiplier for disparities in all the images, rather than dynamically calculating how to best map the disparities in an image to [0...255]. This is so that representations of relative distances are fixed across all images in the dataset.}

For each pixel in the image in turn, the $n$ data points are concatenated into a series of values, and appended to the single column image vector. The result is a single vector of length $n \times m$, where $m$ is the total number of pixels in the image.

For the training phase of the PCA processing, the average image vector for the
training set is computed, and subtracted from each image vector to create the normalised vectors. These vectors
are put into a matrix (one column per image), and the covariance matrix and
eigenvectors are calculated, by means of the OpenCV PCA library functions (which
helps a lot with matching speed).

The eigenvectors with the highest associated eigenvalues are kept; how many to
keep is a user-configurable parameter. The (normalised) vectors form an
orthonormal basis for the eigenspace. Each training image is projected into this
eigenspace, and for each class of images (i.e. each person), the average image
vector (in the eigenspace) is computed, as well as the maximum distance between
the average image and the original training images. The maximum distance
(multiplied by a user-configurable factor) is used as a cutoff distance when
matching; images that do not match any classes within the cutoff distance are
considered a no-match.

To test the accuracy of the classification, each training image is projected
into the eigenspace and back; the error between the original image and the
reconstructed image (as the square of the distance) is calculated. The result of
this is shown in the results subsection below.

When matching an image against the training database, the image vector is built
up as for the training step, and the average vector (also from the training
step) is subtracted. The vector is then projected into the eigenspace, and the
distance to each class average vector is computed. The class vector that is
closest to the image being matched is considered a match if it is closer than
its cutoff depth.

%%% This I do not understand: %%%
% We can keep some quantity of image data by calculating the sum of eigenvalues
% for the components we are keeping, divided by the sum of all eigenvalues.


\subsubsection{Preprocessing of images}
\label{sec:preprocess}

`registered'

Before doing PCA analysis on the face images, they are preprocessed to make sure
face features are in (approximately) the same positions on all the images. First
of all, we operate exclusively on rectified images, i.e. stereo images that have
been projected to have corresponding horizontal epipolar lines. The
rectification step helps diminish deformation of the from the later steps of the
preprocessing.

After rectification, face features are manually identified. The features
identified are the outer corners of the eyes, and the point just below the nose
(since these features seem to be in approximately the same position regardless
of facial expression). The average of each feature point over the whole image
training database is then found, and each image is transformed so that it has
the feature points in this average location. This is done by, for each image,
solving for the affine transformation needed to transform the feature point
locations for that image into the average location, and applying that affine
transformation to the whole image. The solution is found using singular value
decomposition of the resulting set of matrices. This, and the following affine
transformation of the image, is done using OpenCV functions.

Following the transformation, the images are cropped to a user-configurable
ratio determined by adding a factor of the vertical and horizontal distances
between the feature points. An ellipse is then fitted to the image (the maximal
ellipse that will fit inside the image), and everything outside this ellipse is
blacked out. Finally, the image is scaled down to a fixed with (also
user-configurable), to speed up the processing.

Finally, we apply our stereo matching implementation to the processed images to
yield the depth map that is (optionally) used in the PCA matching.

\subsubsection{Testing results}
